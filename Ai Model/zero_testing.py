"""
Hydrogen Site Selection & Portfolio Optimization - ML Ground Zero (COMPLETE FIXED VERSION)
Save this as: zero_testing.py
Run: python zero_testing.py
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import shap
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.optimize import minimize
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Try to import cvxpy, but don't fail if it's not available
try:
    import cvxpy as cp
    CVXPY_AVAILABLE = True
except ImportError:
    CVXPY_AVAILABLE = False
    print("‚ö†Ô∏è CVXPY not available, using scipy optimization only")

# Set style for visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

class HydrogenSiteOptimizer:
    """Complete ML system for hydrogen site selection and portfolio optimization"""
    
    def __init__(self, data_path: str = 'renewable_hydrogen_dataset_2535.csv'):
        """Initialize the optimizer with data"""
        self.data_path = data_path
        self.df = None
        self.model = None
        self.scaler = StandardScaler()
        self.shap_explainer = None
        self.shap_values = None
        self.feature_columns = None
        self.target_column = 'Feasibility_Score'
        
    def load_and_prepare_data(self):
        """Load and prepare the dataset"""
        print("üìä Loading dataset...")
        self.df = pd.read_csv(self.data_path)
        
        # Data overview
        print(f"Dataset shape: {self.df.shape}")
        print(f"Columns: {self.df.columns.tolist()}")
        
        # Handle any missing values
        self.df = self.df.dropna()
        
        # Define feature columns (excluding target and identifiers)
        self.feature_columns = [
            'Solar_Irradiance_kWh/m¬≤/day', 'Temperature_C', 'Wind_Speed_m/s',
            'PV_Power_kW', 'Wind_Power_kW', 'Electrolyzer_Efficiency_%',
            'Hydrogen_Production_kg/day', 'Desalination_Power_kW', 
            'System_Efficiency_%'
        ]
        
        # Add derived features
        self.df['Total_Renewable_Power_kW'] = self.df['PV_Power_kW'] + self.df['Wind_Power_kW']
        self.df['Solar_Wind_Ratio'] = self.df['PV_Power_kW'] / (self.df['Wind_Power_kW'] + 1)
        self.df['Power_per_H2'] = self.df['Total_Renewable_Power_kW'] / (self.df['Hydrogen_Production_kg/day'] + 1)
        
        # Add location-based features (simulate for India regions)
        np.random.seed(42)  # For reproducibility
        self.df['Latitude_India'] = 20 + np.random.rand(len(self.df)) * 15  # 20-35¬∞N
        self.df['Longitude_India'] = 68 + np.random.rand(len(self.df)) * 20  # 68-88¬∞E
        
        # Simulate state assignments
        states = ['Gujarat', 'Karnataka', 'Rajasthan', 'Tamil Nadu', 'Maharashtra']
        np.random.seed(42)
        self.df['State'] = np.random.choice(states, len(self.df))
        
        # Update feature columns with new features
        self.feature_columns.extend(['Total_Renewable_Power_kW', 'Solar_Wind_Ratio', 'Power_per_H2'])
        
        print(f"‚úÖ Data loaded: {len(self.df)} sites")
        return self.df
    
    def calculate_lcoh(self, capex_electrolyzer=800, opex_factor=0.02, discount_rate=0.08, lifetime=20):
        """
        Calculate Levelized Cost of Hydrogen (LCOH) for each site
        
        Parameters:
        - capex_electrolyzer: $/kW for electrolyzer
        - opex_factor: % of CAPEX for annual O&M
        - discount_rate: for NPV calculation
        - lifetime: project lifetime in years
        """
        print("\nüí∞ Calculating LCOH for all sites...")
        
        # Total CAPEX estimation
        self.df['CAPEX_Total'] = (
            self.df['PV_Power_kW'] * 1000 +  # Solar CAPEX
            self.df['Wind_Power_kW'] * 1500 +  # Wind CAPEX
            self.df['Hydrogen_Production_kg/day'] * capex_electrolyzer * 10  # Electrolyzer sizing
        )
        
        # Annual OPEX
        self.df['OPEX_Annual'] = self.df['CAPEX_Total'] * opex_factor
        
        # Annual hydrogen production
        self.df['H2_Annual_kg'] = self.df['Hydrogen_Production_kg/day'] * 350  # 350 operating days
        
        # NPV factor
        npv_factor = sum([1/(1+discount_rate)**i for i in range(1, lifetime+1)])
        
        # LCOH calculation (avoid division by zero)
        self.df['LCOH_USD_per_kg'] = np.where(
            self.df['H2_Annual_kg'] > 0,
            (self.df['CAPEX_Total'] + self.df['OPEX_Annual'] * npv_factor) / 
            (self.df['H2_Annual_kg'] * npv_factor),
            np.inf
        )
        
        # Add LCOH bands
        self.df['LCOH_Band'] = pd.cut(self.df['LCOH_USD_per_kg'], 
                                       bins=[0, 2, 3, 4, 5, 100],
                                       labels=['Ultra Low (<$2)', 'Low ($2-3)', 
                                              'Medium ($3-4)', 'High ($4-5)', 'Very High (>$5)'])
        
        print(f"LCOH Range: ${self.df['LCOH_USD_per_kg'].min():.2f} - ${self.df['LCOH_USD_per_kg'].max():.2f}")
        return self.df['LCOH_USD_per_kg']
    
    def train_ml_models(self, test_size=0.2, random_state=42):
        """Train multiple ML models and compare performance"""
        print("\nü§ñ Training ML Models...")
        
        X = self.df[self.feature_columns]
        y = self.df[self.target_column]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Models to test
        models = {
            'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),
            'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)
        }
        
        results = {}
        
        for name, model in models.items():
            try:
                # Train
                model.fit(X_train_scaled, y_train)
                
                # Predict
                y_pred = model.predict(X_test_scaled)
                
                # Evaluate
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # Cross-validation
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
                
                results[name] = {
                    'model': model,
                    'rmse': rmse,
                    'mae': mae,
                    'r2': r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                print(f"\n{name}:")
                print(f"  RMSE: {rmse:.4f}")
                print(f"  MAE: {mae:.4f}")
                print(f"  R¬≤: {r2:.4f}")
                print(f"  CV R¬≤ (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è {name} failed: {str(e)}")
                continue
        
        # Select best model
        if results:
            best_model_name = max(results, key=lambda x: results[x]['r2'])
            self.model = results[best_model_name]['model']
            print(f"\n‚ú® Best Model: {best_model_name}")
        else:
            # Fallback to simple Random Forest
            self.model = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)
            self.model.fit(X_train_scaled, y_train)
            print("\n‚ú® Using fallback Random Forest model")
        
        return results
    
    def calculate_shap_explanations(self, sample_size=100):
        """Calculate SHAP values for model interpretability"""
        print("\nüîç Calculating SHAP Explanations...")
        
        if self.model is None:
            raise ValueError("Model not trained yet. Run train_ml_models() first.")
        
        try:
            # Prepare data
            X = self.df[self.feature_columns]
            X_scaled = self.scaler.transform(X)
            
            # Use sample for faster computation
            if len(X_scaled) > sample_size:
                sample_idx = np.random.choice(len(X_scaled), sample_size, replace=False)
                X_sample = X_scaled[sample_idx]
            else:
                X_sample = X_scaled
            
            # Create SHAP explainer
            self.shap_explainer = shap.Explainer(self.model, X_sample)
            self.shap_values = self.shap_explainer(X_sample)
            
            # Calculate feature importance
            shap_importance = pd.DataFrame({
                'feature': self.feature_columns,
                'importance': np.abs(self.shap_values.values).mean(axis=0)
            }).sort_values('importance', ascending=False)
            
            print("\nüìä Feature Importance (SHAP):")
            for idx, row in shap_importance.head(10).iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
            
            return self.shap_values, shap_importance
        except Exception as e:
            print(f"  ‚ö†Ô∏è SHAP calculation failed: {str(e)}")
            print("  Using model feature importance instead...")
            
            # Fallback to model's built-in feature importance
            if hasattr(self.model, 'feature_importances_'):
                importance = self.model.feature_importances_
                shap_importance = pd.DataFrame({
                    'feature': self.feature_columns,
                    'importance': importance
                }).sort_values('importance', ascending=False)
                
                print("\nüìä Feature Importance (Model):")
                for idx, row in shap_importance.head(10).iterrows():
                    print(f"  {row['feature']}: {row['importance']:.4f}")
                
                return None, shap_importance
            else:
                return None, None
    
    def create_composite_suitability_index(self):
        """Create composite suitability index"""
        print("\nüìà Creating Composite Suitability Index...")
        
        # Use SHAP values if available, otherwise use model predictions
        if self.shap_values is not None:
            # Get mean absolute SHAP values as weights
            shap_weights = np.abs(self.shap_values.values).mean(axis=0)
            shap_weights = shap_weights / shap_weights.sum()  # Normalize
        elif hasattr(self.model, 'feature_importances_'):
            # Use model feature importances
            shap_weights = self.model.feature_importances_
            shap_weights = shap_weights / shap_weights.sum()
        else:
            # Equal weights as fallback
            shap_weights = np.ones(len(self.feature_columns)) / len(self.feature_columns)
        
        # Calculate composite index
        X = self.df[self.feature_columns]
        X_normalized = (X - X.min()) / (X.max() - X.min() + 0.0001)  # Avoid division by zero
        
        self.df['Composite_Suitability_Index'] = X_normalized.dot(shap_weights)
        
        # Create suitability bands
        self.df['Suitability_Band'] = pd.qcut(self.df['Composite_Suitability_Index'], 
                                               q=5, 
                                               labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        
        print(f"Composite Index Range: {self.df['Composite_Suitability_Index'].min():.3f} - {self.df['Composite_Suitability_Index'].max():.3f}")
        
        return self.df['Composite_Suitability_Index']
    
    def portfolio_optimization_markowitz(self, budget=100_000_000, num_sites=20, risk_aversion=0.5):
        """
        Portfolio optimization using scipy (works without CVXPY issues)
        
        Parameters:
        - budget: Total investment budget (INR)
        - num_sites: Maximum number of sites to select
        - risk_aversion: Risk preference (0=risk-seeking, 1=risk-averse)
        """
        print(f"\nüíº Portfolio Optimization (Budget: ‚Çπ{budget/1e7:.1f} Cr, Sites: {num_sites})")
        
        # Calculate returns (inverse of LCOH - lower cost = higher return)
        if 'LCOH_USD_per_kg' not in self.df.columns:
            self.calculate_lcoh()
        
        # Expected returns (based on hydrogen production and price)
        h2_price = 3.5  # USD/kg assumed market price
        
        # Avoid division by zero
        self.df['Expected_Return'] = np.where(
            self.df['CAPEX_Total'] > 0,
            (h2_price - self.df['LCOH_USD_per_kg']) * self.df['H2_Annual_kg'] / self.df['CAPEX_Total'],
            0
        )
        
        # Select top sites by composite index
        top_sites = self.df.nlargest(min(num_sites * 2, len(self.df)), 'Composite_Suitability_Index')
        
        # Filter out sites with invalid returns
        top_sites = top_sites[np.isfinite(top_sites['Expected_Return'])]
        
        if len(top_sites) < num_sites:
            print(f"  ‚ö†Ô∏è Only {len(top_sites)} valid sites available")
            num_sites = len(top_sites)
        
        # Returns and covariance matrix
        returns = top_sites['Expected_Return'].values
        n_sites = len(returns)
        
        # Simulate return variance based on renewable variability
        return_std = np.abs(returns) * 0.2 + 0.001  # Add small constant to avoid zero
        
        # Create correlation matrix
        correlation_matrix = np.eye(n_sites)
        for i in range(n_sites):
            for j in range(i+1, n_sites):
                # Distance-based correlation
                dist = np.sqrt(
                    (top_sites.iloc[i]['Latitude_India'] - top_sites.iloc[j]['Latitude_India'])**2 +
                    (top_sites.iloc[i]['Longitude_India'] - top_sites.iloc[j]['Longitude_India'])**2
                )
                correlation = max(0, 1 - dist/50)
                correlation_matrix[i, j] = correlation_matrix[j, i] = correlation
        
        # Covariance matrix
        cov_matrix = np.outer(return_std, return_std) * correlation_matrix
        cov_matrix = cov_matrix + np.eye(n_sites) * 1e-8  # Regularization
        
        # Use scipy optimization
        def objective(weights):
            portfolio_return = np.dot(returns, weights)
            portfolio_variance = np.dot(weights.T, np.dot(cov_matrix, weights))
            return -(portfolio_return - risk_aversion * portfolio_variance)
        
        # Constraints
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},  # Sum to 1
        ]
        
        # Bounds
        bounds = tuple((0, 1) for _ in range(n_sites))
        
        # Initial guess
        x0 = np.ones(n_sites) / n_sites
        
        # Optimize
        result = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=constraints)
        
        if result.success:
            optimal_weights = result.x
            
            # Filter out very small weights
            threshold = 0.001
            significant_weights = optimal_weights > threshold
            
            # Keep only top num_sites
            if np.sum(significant_weights) > num_sites:
                sorted_indices = np.argsort(optimal_weights)[::-1][:num_sites]
                new_weights = np.zeros(n_sites)
                new_weights[sorted_indices] = optimal_weights[sorted_indices]
                new_weights = new_weights / new_weights.sum()
                optimal_weights = new_weights
                significant_weights = optimal_weights > threshold
            
            selected_sites = top_sites.iloc[significant_weights].copy()
            selected_weights = optimal_weights[significant_weights]
            selected_weights = selected_weights / selected_weights.sum()
            
            selected_sites['Allocation_%'] = selected_weights * 100
            selected_sites['Investment_INR'] = selected_sites['Allocation_%'] * budget / 100
            
            # Portfolio metrics
            portfolio_return_value = float(np.dot(returns[significant_weights], selected_weights))
            selected_cov = cov_matrix[np.ix_(significant_weights, significant_weights)]
            portfolio_variance_value = float(np.dot(selected_weights, np.dot(selected_cov, selected_weights)))
            portfolio_risk_value = np.sqrt(portfolio_variance_value)
            sharpe_ratio = portfolio_return_value / portfolio_risk_value if portfolio_risk_value > 0 else 0
            
            print(f"\nüìä Optimal Portfolio:")
            print(f"  Expected Return: {portfolio_return_value:.2%}")
            print(f"  Risk (Std Dev): {portfolio_risk_value:.2%}")
            print(f"  Sharpe Ratio: {sharpe_ratio:.2f}")
            print(f"  Sites Selected: {len(selected_sites)}")
            
            # Show top allocations
            print(f"\n  Top 5 Allocations:")
            for idx, row in selected_sites.head(5).iterrows():
                print(f"    {row['City']}: {row['Allocation_%']:.1f}% (‚Çπ{row['Investment_INR']/1e6:.1f}M)")
            
            return {
                'selected_sites': selected_sites,
                'weights': selected_weights,
                'expected_return': portfolio_return_value,
                'risk': portfolio_risk_value,
                'sharpe_ratio': sharpe_ratio
            }
        else:
            print(f"  ‚ö†Ô∏è Optimization failed: {result.message}")
            print("  Using equal-weight portfolio as fallback...")
            
            # Fallback: equal weights for top sites
            selected_sites = top_sites.head(num_sites).copy()
            equal_weights = np.ones(len(selected_sites)) / len(selected_sites)
            
            selected_sites['Allocation_%'] = equal_weights * 100
            selected_sites['Investment_INR'] = selected_sites['Allocation_%'] * budget / 100
            
            portfolio_return_value = float(np.mean(selected_sites['Expected_Return']))
            portfolio_risk_value = float(np.std(selected_sites['Expected_Return']))
            sharpe_ratio = portfolio_return_value / portfolio_risk_value if portfolio_risk_value > 0 else 0
            
            print(f"\nüìä Equal-Weight Portfolio (Fallback):")
            print(f"  Expected Return: {portfolio_return_value:.2%}")
            print(f"  Risk (Std Dev): {portfolio_risk_value:.2%}")
            print(f"  Sharpe Ratio: {sharpe_ratio:.2f}")
            print(f"  Sites Selected: {len(selected_sites)}")
            
            return {
                'selected_sites': selected_sites,
                'weights': equal_weights,
                'expected_return': portfolio_return_value,
                'risk': portfolio_risk_value,
                'sharpe_ratio': sharpe_ratio
            }
    
    def generate_efficient_frontier(self, num_points=20):
        """Generate efficient frontier for different risk levels"""
        print("\nüìâ Generating Efficient Frontier...")
        
        returns_list = []
        risks_list = []
        
        for i, risk_aversion in enumerate(np.linspace(0.01, 0.99, num_points)):
            try:
                print(f"  Computing point {i+1}/{num_points}...", end='\r')
                result = self.portfolio_optimization_markowitz(risk_aversion=risk_aversion)
                if result:
                    returns_list.append(result['expected_return'])
                    risks_list.append(result['risk'])
            except Exception as e:
                continue
        
        print(f"\n  ‚úÖ Generated {len(returns_list)} frontier points")
        
        return np.array(risks_list), np.array(returns_list)
    
    def stress_test_scenarios(self):
        """Run stress tests on different scenarios"""
        print("\n‚ö° Running Stress Test Scenarios...")
        
        scenarios = {
            'Base Case': {'capex_change': 1.0, 'h2_price': 3.5, 'efficiency_change': 1.0},
            'High CAPEX': {'capex_change': 1.3, 'h2_price': 3.5, 'efficiency_change': 1.0},
            'Low H2 Price': {'capex_change': 1.0, 'h2_price': 2.5, 'efficiency_change': 1.0},
            'Tech Improvement': {'capex_change': 0.8, 'h2_price': 3.5, 'efficiency_change': 1.2},
            'Worst Case': {'capex_change': 1.5, 'h2_price': 2.0, 'efficiency_change': 0.9}
        }
        
        results = {}
        
        # Save original values
        original_capex = self.df['CAPEX_Total'].copy() if 'CAPEX_Total' in self.df else None
        original_efficiency = self.df['Electrolyzer_Efficiency_%'].copy()
        
        for scenario_name, params in scenarios.items():
            try:
                print(f"\n  Testing: {scenario_name}...")
                
                # Adjust parameters
                if original_capex is not None:
                    self.df['CAPEX_Total'] = original_capex * params['capex_change']
                self.df['Electrolyzer_Efficiency_%'] = original_efficiency * params['efficiency_change']
                
                # Recalculate LCOH
                self.calculate_lcoh()
                
                # Run portfolio optimization
                portfolio_result = self.portfolio_optimization_markowitz(num_sites=10)
                
                if portfolio_result:
                    results[scenario_name] = {
                        'return': portfolio_result['expected_return'],
                        'risk': portfolio_result['risk'],
                        'sharpe': portfolio_result['sharpe_ratio'],
                        'num_sites': len(portfolio_result['selected_sites'])
                    }
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è Scenario failed: {str(e)}")
                continue
            finally:
                # Restore original values
                if original_capex is not None:
                    self.df['CAPEX_Total'] = original_capex
                self.df['Electrolyzer_Efficiency_%'] = original_efficiency
        
        # Display results
        if results:
            print("\nüìä Stress Test Results:")
            for scenario, metrics in results.items():
                print(f"\n{scenario}:")
                print(f"  Return: {metrics['return']:.2%}")
                print(f"  Risk: {metrics['risk']:.2%}")
                print(f"  Sharpe: {metrics['sharpe']:.2f}")
                print(f"  Sites: {metrics['num_sites']}")
        
        return results
    
    def get_top_sites_by_state(self, top_n=5):
        """Get top N sites for each state"""
        print(f"\nüó∫Ô∏è Top {top_n} Sites by State:")
        
        for state in self.df['State'].unique():
            state_df = self.df[self.df['State'] == state].nlargest(min(top_n, len(self.df[self.df['State'] == state])), 'Composite_Suitability_Index')
            print(f"\n{state}:")
            for idx, site in state_df.iterrows():
                lcoh = site['LCOH_USD_per_kg'] if 'LCOH_USD_per_kg' in site else 'N/A'
                print(f"  {site['City']}: Score={site['Composite_Suitability_Index']:.3f}, LCOH=${lcoh:.2f}" if lcoh != 'N/A' else f"  {site['City']}: Score={site['Composite_Suitability_Index']:.3f}")
    
    def export_results(self, output_prefix='hydrogen_optimization'):
        """Export all results to files"""
        print(f"\nüíæ Exporting results...")
        
        try:
            # Export main dataframe with all calculations
            self.df.to_csv(f'{output_prefix}_full_results.csv', index=False)
            
            # Export top sites
            top_sites = self.df.nlargest(min(50, len(self.df)), 'Composite_Suitability_Index')
            top_sites.to_csv(f'{output_prefix}_top_sites.csv', index=False)
            
            print(f"‚úÖ Results exported with prefix: {output_prefix}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è Export failed: {str(e)}")

# ============================================================================
# MAIN EXECUTION - Ground Zero Testing
# ============================================================================

def run_complete_pipeline():
    """Run the complete ML pipeline - Ground Zero Testing"""
    
    print("="*80)
    print("üöÄ HYDROGEN SITE OPTIMIZATION - ML GROUND ZERO (FIXED VERSION)")
    print("="*80)
    
    # Initialize optimizer
    optimizer = HydrogenSiteOptimizer('renewable_hydrogen_dataset_2535.csv')
    
    # 1. Load and prepare data
    optimizer.load_and_prepare_data()
    
    # 2. Calculate LCOH
    optimizer.calculate_lcoh()
    
    # 3. Train ML models
    model_results = optimizer.train_ml_models()
    
    # 4. Calculate SHAP explanations (with fallback)
    shap_values, shap_importance = optimizer.calculate_shap_explanations()
    
    # 5. Create composite suitability index
    optimizer.create_composite_suitability_index()
    
    # 6. Portfolio optimization (now working without CVXPY issues)
    portfolio_result = optimizer.portfolio_optimization_markowitz(
        budget=100_000_000,  # 10 Crore INR
        num_sites=15,
        risk_aversion=0.5
    )
    
    # 7. Generate efficient frontier (simplified)
    print("\nüìâ Generating Efficient Frontier (simplified)...")
    risks, returns = [], []
    for risk_aversion in [0.1, 0.3, 0.5, 0.7, 0.9]:
        result = optimizer.portfolio_optimization_markowitz(num_sites=10, risk_aversion=risk_aversion)
        if result:
            risks.append(result['risk'])
            returns.append(result['expected_return'])
    print(f"  ‚úÖ Generated {len(risks)} frontier points")
    
    # 8. Stress testing (simplified)
    stress_results = optimizer.stress_test_scenarios()
    
    # 9. Top sites by state
    optimizer.get_top_sites_by_state()
    
    # 10. Export results
    optimizer.export_results()
    
    print("\n" + "="*80)
    print("‚úÖ PIPELINE COMPLETE - All functionalities tested successfully!")
    print("="*80)
    
    return optimizer

# ============================================================================
# QUICK TEST FUNCTIONS
# ============================================================================

def test_single_site_prediction(optimizer, site_features):
    """Test prediction for a single site"""
    site_scaled = optimizer.scaler.transform([site_features])
    prediction = optimizer.model.predict(site_scaled)[0]
    return prediction

def test_scenario_analysis(optimizer, capex_reduction=0.2, h2_price_increase=0.3):
    """Test specific scenario"""
    print(f"\nüî¨ Testing Scenario: CAPEX -{capex_reduction*100}%, H2 Price +{h2_price_increase*100}%")
    
    # Modify parameters
    original_capex = optimizer.df['CAPEX_Total'].copy()
    optimizer.df['CAPEX_Total'] = original_capex * (1 - capex_reduction)
    
    # Recalculate and optimize
    optimizer.calculate_lcoh()
    result = optimizer.portfolio_optimization_markowitz(num_sites=10)
    
    # Restore
    optimizer.df['CAPEX_Total'] = original_capex
    
    return result

def test_api_endpoints_simulation():
    """Simulate API endpoint responses"""
    print("\nüåê API Endpoint Simulation:")
    
    endpoints = {
        "/ml/score/batch": "POST - Batch scoring for multiple sites",
        "/ml/shap/explain": "POST - SHAP explanations for sites",
        "/ml/lcoh/calculate": "POST - LCOH calculation with parameters",
        "/ml/portfolio/optimize": "POST - Portfolio optimization",
        "/ml/portfolio/stress-test": "POST - Stress testing scenarios",
        "/ml/sites/top": "GET - Top sites by region",
        "/ml/frontier/efficient": "GET - Efficient frontier data"
    }
    
    for endpoint, description in endpoints.items():
        print(f"  {endpoint}: {description}")
    
    return endpoints

# ============================================================================
# MAIN ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    try:
        # Run complete pipeline
        optimizer = run_complete_pipeline()
        
        # Additional tests
        print("\n" + "="*80)
        print("üß™ ADDITIONAL TESTING")
        print("="*80)
        
        # Test single site prediction
        if optimizer.model and len(optimizer.feature_columns) > 0:
            sample_features = optimizer.df[optimizer.feature_columns].iloc[0].values
            prediction = test_single_site_prediction(optimizer, sample_features)
            print(f"\n‚ú® Single Site Prediction: {prediction:.3f}")
        
        # Test scenario
        scenario_result = test_scenario_analysis(optimizer, capex_reduction=0.1, h2_price_increase=0.2)
        
        # Test API endpoints
        api_endpoints = test_api_endpoints_simulation()
        
        print("\n" + "="*80)
        print("üéØ ML GROUND ZERO TESTING COMPLETE!")
        print("Ready for integration with Spring Backend and React Frontend")
        print("="*80)
        
    except Exception as e:
        print(f"\n‚ùå Error in main execution: {str(e)}")
        print("But core functionality should still work!")
        print("\n" + "="*80)
        print("üí° TIP: Check the exported CSV files for results")
        print("="*80)